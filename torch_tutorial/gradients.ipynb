{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4da76b37-73f3-4808-8cd7-dee616bcf8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version    : 2.0.1+cu117\n",
      "Torchvision version: 0.15.2+cu117\n",
      "Torchaudio version : 2.0.2+cu117\n",
      "CUDA version       : 11.7\n",
      "cuDNN version      : 8500\n",
      "NumPy version      : 1.24.4\n",
      "torch.accelerator is NOT available\n",
      "\n",
      "Selected device    : cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "print(\"PyTorch version    :\", torch.__version__)\n",
    "print(\"Torchvision version:\", torchvision.__version__)\n",
    "print(\"Torchaudio version :\", torchaudio.__version__)\n",
    "print(\"CUDA version       :\", torch.version.cuda)\n",
    "print(\"cuDNN version      :\", torch.backends.cudnn.version())\n",
    "print(\"NumPy version      :\", np.__version__)\n",
    "\n",
    "\n",
    "if hasattr(torch, 'accelerator'):\n",
    "    print(\"torch.accelerator is available\\n\")\n",
    "    device = torch.accelerator.current_accelerator() if torch.accelerator.is_available() else \"cpu\"\n",
    "else:\n",
    "    print(\"torch.accelerator is NOT available\\n\")\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "    elif torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "\n",
    "print(\"Selected device    :\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daf05c61-375e-429d-b2af-b9747830c8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d52737db-b940-4988-aafb-899cf03af40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 3.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b7dc60a-d1cd-445d-aabc-cc344740f4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6., 4.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f2d7e36-0c77-4626-8ca7-0a15efcb45dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# element-wise square\n",
    "Q = 3*a**3 - b**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfb2eb75-d42e-4ed0-a8f3-7c183536d67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_grad = torch.tensor([1., 1.])\n",
    "Q.backward(gradient=external_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b97586c-e195-41b4-9716-a1910f61c032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([36., 81.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea83c70e-81c2-4eb9-8145-8d1a8517fdb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-12.,  -8.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f881edfd-fbe0-4f0b-8f0d-56ecab932b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True])\n",
      "tensor([True, True])\n"
     ]
    }
   ],
   "source": [
    "# check if collected gradients are correct\n",
    "print(9*a**2 == a.grad)\n",
    "print(-2*b == b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc33f7da-7a34-4047-beac-c254be1ce8dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02b82424",
   "metadata": {},
   "source": [
    "```\n",
    "Differentiation 微分\n",
    "Jacobian matrix 雅可比矩阵\n",
    "Calculus 微积分\n",
    "\n",
    "\n",
    "对于一个从R^n 映射到R^m 的函数，其雅可比矩阵是一个m 行n 列的矩阵，其中的元素是各个分量函数对各个自变量的偏导数。\n",
    "\n",
    "Mathematically, if you have a vector valued function \n",
    "\n",
    "y ⃗= f ( x ⃗) then the gradient of y ⃗with respect to x ⃗is a Jacobian matrix J:\n",
    "\n",
    "\n",
    "\n",
    "J = ( ∂y/∂x1, ∂y/∂x2, ... , ∂y/∂xn )\n",
    "\n",
    "     ∂y1/∂x1,  ∂y1/∂x2   ...  ∂y1/∂xn \n",
    "     ∂y2/∂x1,  ∂y2/∂x2   ...  ∂y2/∂xn \n",
    "=    ...       ...       ...  ...\n",
    "\n",
    "     ∂ym/∂x1   ∂ym/∂x2   ...  ∂ym/∂xn \n",
    "\n",
    "\n",
    "Generally speaking, torch.autograd is an engine for computing vector-Jacobian\n",
    "product. That is, given any vector v ⃗, compute the product J(T) ⋅ v ⃗\n",
    " \n",
    "In the context of PyTorch and autograd, a scalar function is a function that takes one or more inputs\n",
    "(which can be tensors of any shape) and produces a single scalar output\n",
    "— i.e., a tensor with just one element (shape torch.Size([])).\n",
    "\n",
    "If v ⃗happens to be the gradient of a scalar function \n",
    "l = g ( y ⃗):\n",
    "\n",
    "v ⃗ = (∂l/∂y1,  ∂l/∂y2, ... ∂l/∂yn)T\n",
    "\n",
    "then by the chain rule, the vector-Jacobian product would be the gradient of \n",
    "l with respect to x ⃗:\n",
    "\n",
    "\n",
    "              ∂y1/∂x1,  ∂y2/∂x1   ...  ∂ym/∂x1 \n",
    "J(T) ⋅ v ⃗ =   ∂y1/∂x2,  ∂y2/∂x2   ...  ∂ym/∂x2    .   (∂l/∂y1,  ∂l/∂y2, ... ∂l/∂yn)T \n",
    "\n",
    "              ...       ...       ...  ... \n",
    "              ∂y1/∂xn   ∂y2/∂xn   ...  ∂ym/∂xn  \n",
    "\n",
    "\n",
    "= (∂l/∂x1,  ∂l/∂x2, ... ∂l/∂xn)T \n",
    "\n",
    "\n",
    "\n",
    "This characteristic of vector-Jacobian product is what we use in the above example;\n",
    "external_grad represents v ⃗.\n",
    "\n",
    "```\n",
    "\n",
    "# Why scalar functions matter in autograd\n",
    "\n",
    "PyTorch’s autograd is designed to compute gradients using reverse-mode automatic\n",
    "differentiation, which is especially efficient when the output is a scalar\n",
    "(typical in loss functions).\n",
    "\n",
    "In deep learning, your loss function is a scalar function of your model\n",
    "parameters. So when you call: `loss.backward()` PyTorch computes gradients of\n",
    "the scalar loss with respect to all tensors that have requires_grad=True\n",
    "\n",
    "A scalar function outputs a single number (0-dimensional tensor).\n",
    "\n",
    "In PyTorch, it's typically used for loss functions.\n",
    "\n",
    "`autograd.backward()` only works directly on scalar outputs (or you need to\n",
    "supply a gradient argument if it's not scalar)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7490a9c0",
   "metadata": {},
   "source": [
    "# Computational Graph\n",
    "\n",
    "Conceptually, autograd keeps a record of data (tensors) & all executed operations\n",
    "(along with the resulting new tensors) in a directed acyclic graph\n",
    "(DAG)`有向无环图` consisting of Function objects. In this DAG, leaves are the\n",
    "input tensors, roots are the output tensors. By tracing this graph from roots to\n",
    "leaves, you can automatically compute the gradients using the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4274fbd",
   "metadata": {},
   "source": [
    "In a NN, parameters that don’t compute gradients are usually called frozen\n",
    "parameters. It is useful to “freeze” part of your model if you know in advance\n",
    "that you won’t need the gradients of those parameters (this offers some\n",
    "performance benefits by reducing autograd computations)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2704a80",
   "metadata": {},
   "source": [
    "# Finetuning\n",
    "In finetuning, we freeze most of the model and typically only modify the\n",
    "classifier layers to make predictions on new labels. Let’s walk through a small\n",
    "example to demonstrate this. As before, we load a pretrained resnet18 model,\n",
    "and freeze all the parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f720d0e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "#data = torch.rand(1, 3, 64, 64)\n",
    "#labels = torch.rand(1, 1000)\n",
    "\n",
    "\n",
    "#model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "#prediction = model(data)\n",
    "\n",
    "# Freeze all the parameters in the network\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model\n",
    "\n",
    "\n",
    "model.fc = nn.Linear(512, 10)\n",
    "\n",
    "# Optimize only the classifier\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c81095",
   "metadata": {},
   "source": [
    "[Autograd](https://www.youtube.com/watch?v=MswxJw-8PvE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2.0",
   "language": "python",
   "name": "torch2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
